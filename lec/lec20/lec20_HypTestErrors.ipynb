{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors in Hypothesis Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Students Flipping Fair Coins Conclude Coin is Unfair!\n",
    "\n",
    "Suppose there are 5000 students and each student:\n",
    "* is given a fair coin by the instructor but _they are not told that it is a fair coin_\n",
    "* **collects data** by flipping the coin **160 times** and counts the number of times it lands Heads\n",
    "* runs a **hypothesis test**:\n",
    "    * **Null Hypothesis:** They were given a fair coin and the number of heads observed see is due to chance.\n",
    "    * **Alternative Hypothesis:** The coin is biased and so the number of heads they observed is not due to chance alone.\n",
    "    * **Test Statistic:** abs(num_heads - 80)\n",
    "* Uses the null probability distribution to report their *p-value* and rejects the null hypothesis **if their p-value is less than 0.05**\n",
    "\n",
    "We know that **we gave all of them fair coins**.  How often will they **incorrectly reject the null hypothesis?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Statistic:\n",
    "For this example, since we're interested in ANY difference from fair (ie the coin can be biased towards either heads or tails) we're going to use the test statistic |Num heads -80|.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating the Test Statistic Under the Null Hypothesis\n",
    "Let's write code to simulate the process of one student running this hypothesis test: \n",
    "\n",
    "**Practice:  Simulating flipping a fair coin 160 times and counting number of heads:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate flipping a fair coin 160 times and counting number of heads:\n",
    "from scipy import stats\n",
    "\n",
    "stats.binom.rvs(160, .50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test statistic:  abs(num_heads - numflips/2)\n",
    "\n",
    "#Input number of heads out of num_flips\n",
    "def test_statistic(num_heads, num_flips):\n",
    "    return np.abs(num_heads - num_flips/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_distribution_test_statistic(num_flips, num_simulations):\n",
    "    #Conduct experiment (flip coin num_flips times)\n",
    "    \n",
    "    return np.array([test_statistic(np.random.binomial(num_flips, 0.5), num_flips) for i in range(num_simulations)])\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_distribution_test_statistic(160, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot simulated test distribution:\n",
    "\n",
    "#Here we use a very large number of simulations so that the simulated test \n",
    "# statistic distribution is as close as possible to the theoretical distribution\n",
    "num_sim=100000\n",
    "\n",
    "simulated_statistics = simulate_distribution_test_statistic(160, num_sim)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,3))\n",
    "\n",
    "ax.hist(simulated_statistics,density=True, bins=np.arange(21));\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_xlim(-1,20)\n",
    "ax.set_ylim(0, .15)\n",
    "ax.set_title(\"Simulated Distribution of Test Statistic assuming fair coin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_one_student(num_flips, p_heads, sim_stats, plot=False):\n",
    "    \n",
    "    \n",
    "    #Conduct experiment (flip coin num_flips times)\n",
    "    obs_flips = stats.binom.rvs(num_flips, p_heads)\n",
    "\n",
    "    \n",
    "    # Compute the observed value of the statistic on our actual data\n",
    "    obs_statistic = test_statistic(obs_flips, num_flips)\n",
    "    \n",
    "    p_value = sum(sim_stats>=obs_statistic)/len(sim_stats)\n",
    "    \n",
    "    \n",
    "    if plot==True:\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "        ax.hist(simulated_statistics,density=True, bins=np.arange(21));\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.grid(alpha=0.25)\n",
    "        ax.set_xlim(-1,20)\n",
    "        ax.set_ylim(-.02, .15)\n",
    "        ax.set_title(\"Simulated Distribution of Test Statistic assuming fair coin\")\n",
    "        \n",
    "\n",
    "        #Add a dot for the observed statistic\n",
    "        plt.scatter(obs_statistic, -0.001, color='red', s=70);\n",
    "    \n",
    "   \n",
    "    return obs_statistic, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_one_student(160,0.5,simulated_statistics, plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1: Fair Coin:   \n",
    "Simulate 160 coin tosses by each student, for 5000 students total. About how many of the students conducting this hypothesis test will INCORRECTLY reject the null?\n",
    "That is, how many of the 0 simulations resulted in an empirical p-value <=0.05?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_students = 5000\n",
    "\n",
    "num_flips = 160\n",
    "\n",
    "p_heads = 0.50\n",
    "\n",
    "sim_5000 = np.array([simulate_one_student(num_flips,p_heads, simulated_statistics, plot=False) for i in np.arange(num_students)])\n",
    "\n",
    "\n",
    "print(\"Number of students out of\", str(num_students),\" with p-value <= 0.05 =  \", str(sum(sim_5000[:,1]<=0.05)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Biased Coin\n",
    "Suppose in reality the coin is actually biased such that P(H) = 0.45 (i.e. your null hypothesis is actually wrong).\n",
    "\n",
    "You don't know this, so your null hypothesis remains the same.  Let's look at how often we correctly reject the null in this case.\n",
    "\n",
    "* **Null Hypothesis:** Coins are fair\n",
    "* **Alternative Hypothesis:** Coins are biased.\n",
    "* **Test Statistic:** Absolute difference between number of heads and 50% heads.\n",
    "\n",
    "\n",
    "What percentage of the students conducting this hypothesis test will CORRECTLY reject the null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_students = 1000\n",
    "\n",
    "num_flips = 160\n",
    "\n",
    "p_heads = ...\n",
    "\n",
    "sim_biased = np.array([simulate_one_student(num_flips,p_heads,simulated_statistics, plot=False) for i in np.arange(num_students)])\n",
    "\n",
    "sim_biased[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of students out of\", str(num_students),\" with p-value <= 0.05 =  \", str(sum(sim_biased[:,1]<=0.05)), \" i.e. the power is \", str(sum(sim_biased[:,1]<=0.05)/num_students))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition:  The Power of a hypothesis test is the probability of correctly rejecting the null hypothesis when the alternative hypothesis is true.\n",
    "\n",
    "**Convention: We usually try to design hypothesis tests so that the power is at least 80%.**  \n",
    "Power is affected by \n",
    " - Significance level (p-value cutoff)\n",
    " - Sample size\n",
    " - Effect size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our original scenario above, the power is approximately 22% which is not great from an error standpoint.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Practice**:  If the true probability of heads is 0.45, what sample size do we need to use to correctly reject our null hypothesis (that the probability of heads is is 0.5) at least 80% of the time?  (i.e. what sample size do we need to get a Power of at least 80%)?   Assume we want to keep the same significance level of 0.05.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: \n",
    "\n",
    "num_students = 1000\n",
    "\n",
    "num_flips = ...\n",
    "\n",
    "p_heads = 0.45\n",
    "\n",
    "sim_biased_v2 = np.array([simulate_one_student(num_flips,p_heads,simulated_statistics, plot=False) for i in np.arange(num_students)])\n",
    "\n",
    "print(\"Number of students out of\", str(num_students),\" with p-value <= 0.05 =  \", str(sum(sim_biased_v2[:,1]<=0.05)), \" i.e. the power is \", str(sum(sim_biased_v2[:,1]<=0.05)/num_students))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect Size:\n",
    "**Effect Size**:  The difference between the true proportion and the null hypothesis proportion.\n",
    "#### Experiment with different effect sizes.  How does the power of the test change as you change the effect size ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: \n",
    "\n",
    "num_students = 1000\n",
    "\n",
    "num_flips = 160\n",
    "\n",
    "p_heads = ...\n",
    "\n",
    "sim_biased_v3 = np.array([simulate_one_student(num_flips,p_heads,simulated_statistics, plot=False) for i in np.arange(num_students)])\n",
    "\n",
    "print(\"Number of students out of\", str(num_students),\" with p-value <= 0.05 =  \", str(sum(sim_biased_v3[:,1]<=0.05)), \" i.e. the power is \", str(sum(sim_biased_v3[:,1]<=0.05)/num_students))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Example 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Super Soda Co and the Case of Bad Taste\n",
    "\n",
    "Manufacturers of Super Soda run a taste test and 91 out of 200 tasters prefer Super Soda over its rival.  The boss is upset!  He asks:\n",
    "\n",
    "    Do fewer people prefer Super Soda, or is this just chance?\n",
    "    \n",
    "You run a hypothesis test:\n",
    "\n",
    "* **Null Hypothesis:** Equal proportions of the population prefer Super Soda as Rival and any variability is due to chance.\n",
    "* **Alternative Hypothesis:** Fewer people in the population prefer Super Soda than its Rival.\n",
    "* **Test Statistic:** Number of people who prefer Super Soda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You pick a **significance level (i.e. p-value cutoff) of 0.05**\n",
    "\n",
    "### Null Hypothesis Distribution:\n",
    "What probability distribution models our null hypothesis?  Plot a histogram of this distribution AND a red dot with the observed test statistic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=200\n",
    "k = np.arange(n+1)\n",
    "#Probability for a binomial distribution with p=0.50\n",
    "p = special.comb(n, k)*(0.50**k)*(0.50**(n-k))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(k, p, width=1, ec='white');\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(alpha=0.25)\n",
    "plt.xlim(0,int(n*3/4))\n",
    "plt.title(\"Bin(n=\"+str(n)+\", p=0.50)\");\n",
    "plt.xlabel(\"Number of Heads out of \"+str(n))\n",
    "plt.ylabel(\"Percent per unit\")\n",
    "\n",
    "\n",
    "obs_stat=91\n",
    "\n",
    "#Add a dot for the observed statistic\n",
    "plt.scatter(obs_stat, -0.002, color='red', s=70);\n",
    "#Shade in the p-value   \n",
    "ax.bar(np.arange(n+1)[np.arange(n+1)<=obs_stat], p[np.arange(n+1)<=obs_stat], width=1, ec='white',alpha=0.6);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical P-Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we have the theoretical probability distribution,\n",
    "#the p-value is just the sum of the probabilities for X<=91\n",
    "actual_p_val = sum(p[np.arange(n+1)<=obs_stat])\n",
    "\n",
    "actual_p_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of Test:\n",
    "\n",
    "Since 0.11>0.05, we **fail to reject** the null hypothesis that equal proportions of the population prefer Super Soda as Rival and any variability is due to chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Empirical P-Value:  The Importance of Number of Simulations.\n",
    "\n",
    "What if we didn't know the theoretical distribution?  How does the theoretical p-value compare to simulated p-values?  Suppose we're simulating the null hypothesis (instead of using the actual theoretical distribution).  Let's examine how the number of simulations we use affects how accurate our empirical p-value estimate is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Simulate the number of heads when randomly flipping a fair coin 200 times\n",
    "\n",
    "def simulate_one_count(sample_size):\n",
    "    return np.count_nonzero(np.random.choice(['H', 'T'], sample_size) == 'H')\n",
    "simulate_one_count(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the number of simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the observed test statistic fixed, \n",
    "# we can re-run the test with a new simulation under the null\n",
    "\n",
    "def run_test(num_simulations, sample_size):\n",
    "    counts=np.array([simulate_one_count(sample_size) for i in range(num_simulations)])\n",
    "    return counts\n",
    "\n",
    "counts = run_test(10000, 200)\n",
    "print(\"Actual (theoretical) p-value: \", actual_p_val)\n",
    "print(\"Empirical (simulated) p-value using 10000 simulations: \", np.count_nonzero(counts <= 91)/len(counts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the empirical p-value varies with the\n",
    "# number of siulations:  \n",
    "tests = pd.DataFrame(columns=['simulations', 'p-value for 91'])\n",
    "    \n",
    "for num_sims in [100, 1000, 10000]:\n",
    "    for k in np.arange(50):\n",
    "        counts = run_test(num_sims, 200)\n",
    "        tests.loc[len(tests.index)] = [num_sims, np.count_nonzero(counts <= 91)/len(counts)] \n",
    " \n",
    "tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For larger numbers of simulations, p-values are more consistent\n",
    "# Since a large number of simulations provides a good estimate of the\n",
    "# theoretical distribution of the test statistic under the null hypothesis:\n",
    "\n",
    "sns.histplot(data=tests, x=\"p-value for 91\", hue=\"simulations\", multiple=\"stack\",stat=\"density\")\n",
    "sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "plt.scatter(actual_p_val, -0.002, color='red', s=70);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Power of Test:\n",
    "\n",
    "\n",
    "### Definition:  The Power of a hypothesis test is the probability of correctly rejecting the null hypothesis when the alternative hypothesis is true.\n",
    "\n",
    "**Convention: We usually try to design hypothesis tests so that the power is at least 80%.**  \n",
    "Power is affected by \n",
    " - Significance level (p-value cutoff)\n",
    " - Sample size\n",
    " - Effect size\n",
    " \n",
    "Let's experiment with how these 3 factors can affect the power of the test: \n",
    "\n",
    "Suppose in reality the true proportion of people who prefer Super Soda is 45% (i.e. your null hypothesis is actually wrong).\n",
    "\n",
    "You don't know this, so your null hypothesis remains the same.  Let's look at how often we correctly reject the null in this case.\n",
    "\n",
    "* **Null Hypothesis:** Equal proportions of the population prefer Super Soda as Rival and any variability is due to chance.\n",
    "* **Alternative Hypothesis:** Fewer people in the population prefer Super Soda than its Rival.\n",
    "* **Test Statistic:** Number of people who prefer Super Soda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose that the true proportion of people who prefer Super Soda is 45%\n",
    "true_proportion = 0.45\n",
    "true_distribution = [true_proportion, 1 - true_proportion]\n",
    "true_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taste tests with 200 people will give various numbers of people who prefer Super Soda\n",
    "sample_size = 200\n",
    "\n",
    "np.random.binomial(sample_size, true_distribution).item(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Suppose in the actual population, 45% of people like Super Soda\n",
    "# Let's run a taste test for 200 people, \n",
    "# and calculate the p-value:\n",
    "\n",
    "def run_experiment(num_simulations, sample_size, true_proportion):\n",
    "    # Collect data\n",
    "    true_distribution = [true_proportion, 1 - true_proportion]\n",
    "    taste_test_results = np.random.binomial(sample_size, true_distribution)\n",
    "    observed_stat_from_this_sample = taste_test_results.item(0)\n",
    "    \n",
    "    # Conduct hypothesis test\n",
    "    counts = run_test(num_simulations, sample_size)\n",
    "    p_value = np.count_nonzero(counts <= observed_stat_from_this_sample) / len(counts)\n",
    "    return p_value\n",
    "\n",
    "(\"P-value from one experiment:\", run_experiment(10000, 200, 0.45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's imagine running our taste test over and over again to see how often \n",
    "# we correctly reject the null:\n",
    "\n",
    "true_proportion = 0.45\n",
    "sample_size = 650\n",
    "\n",
    "p_values=np.array([run_experiment(1000,sample_size,true_proportion) for i in range(100)])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    " \n",
    "ax.hist(p_values,bins=np.arange(0,1, .05));\n",
    "ax.hist(p_values[p_values<=0.05],bins=1,label=\"Power: \"+str(100*np.mean(p_values <= 0.05))+\"% of tests correctly reject null\")\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(alpha=0.25)\n",
    "plt.title(\"Sample Size:\"+str(sample_size)+\",Null:p=0.5, Alt:p=\"+str(true_proportion));\n",
    "plt.xlabel(\"p-values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Sample Size for Power of 80%:\n",
    "#### If the true proportion of people who like Super Soda is 0.45, what Sample Size do we need to use to correctly reject our null hypothesis (that the proportion is 0.5) at least 80% of the time?  (i.e. what sample size do we need to get a Power of at least 80%)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_proportion = 0.45\n",
    "\n",
    "sample_size = 600\n",
    "\n",
    "p_values=np.array([run_experiment(1000,sample_size,true_proportion) for i in range(100)])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    " \n",
    "ax.hist(p_values,bins=np.arange(0,1, .05));\n",
    "ax.hist(p_values[p_values<=0.05],bins=1,label=\"Power: \"+str(100*np.mean(p_values <= 0.05))+\"% of tests correctly reject null\")\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(alpha=0.25)\n",
    "plt.title(\"Sample Size:\"+str(sample_size)+\",Null:p=0.5, Alt:p=\"+str(true_proportion));\n",
    "plt.xlabel(\"p-values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect Size:\n",
    "**Effect Size**:  The difference between the true proportion and the null hypothesis proportion.\n",
    "#### Experiment with different effect sizes.  How does the power of the test change as you change the effect size ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_proportion = .48\n",
    "sample_size = 200\n",
    "\n",
    "p_values=np.array([run_experiment(1000,sample_size,true_proportion) for i in range(100)])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    " \n",
    "ax.hist(p_values,bins=np.arange(0,1, .05));\n",
    "ax.hist(p_values[p_values<=0.05],bins=1,label=\"Power: \"+str(100*np.mean(p_values <= 0.05))+\"% of tests correctly reject null\")\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(alpha=0.25)\n",
    "plt.title(\"Sample Size:\"+str(sample_size)+\",Null:p=0.5, Alt:p=\"+str(true_proportion));\n",
    "plt.xlabel(\"p-values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the effect size larger and keeping the same sample size the same (200) leads to a larger power of the test (we're more likely to be able to correctly reject the null). \n",
    "Similarly, if we decrease the effect size, we will need a larger sample size to keep the same level of power. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
